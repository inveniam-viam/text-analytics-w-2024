{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "How to represent words in such a way that a computer can process them (with a view to later training a neural network that can understand their meaning.)\n",
    "\n",
    "Consider the word L-I-S-T-E-N.\n",
    "\n",
    "The word \"Listen\" is made up of a sequence of letters.\n",
    "\n",
    "These letters can be represented by numbers using an *encoding* scheme.\n",
    "\n",
    "L (076) I (073) S(083) T(084) E(069) N(078) *(Encoding in ASCII)*\n",
    "\n",
    "The word S-I-L-E-N-T has the same letters, although in a different order.\n",
    "\n",
    "This makes it hard to understand the sentiment of a given word just by the letters in it.\n",
    "\n",
    "**Might be a better idea to encode words rather than letters, then.**\n",
    "\n",
    "Consider the sentence \"I love my dog\"\n",
    "\n",
    "- word \"I\" could be 001\n",
    "- word \"love\" could be 002\n",
    "- word \"my\" could be 003\n",
    "- word \"dog\" could be 004\n",
    "\n",
    "If we take the case of the sentence \"I love my cat\"\n",
    "\n",
    "- word \"I\" is 001\n",
    "- word \"love\" is 002\n",
    "- word \"my\" is 003\n",
    "- word \"dog\" could be 005\n",
    "\n",
    "So these two sentences are now equivalent to:\n",
    "\n",
    "\"I love my dog\" - \"001 002 003 004\"\n",
    "\"I love my cat\" - \"001 002 003 005\"\n",
    "\n",
    "This already shows some form of similarity in between the sentences, as is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer   # Tokenizer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences: list[str] = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)  # num_words: maximum number of words to keep\n",
    "\n",
    "# num_words - maximum number of words to keep\n",
    "# if we were to tokenize 100 books but only wanted the most frequently occurring 100 words\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index   # full list of words are stored in word_index\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Sentences into Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences: list[str] = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"Do you think my dog is amazing?\"\n",
    "]\n",
    "\n",
    "# here, we have added a new sentence that is also fundamentally different in length\n",
    "# compared to the other sentences in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "# creates sequences of tokens representing each sentence\n",
    "\n",
    "print(word_index)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this above case, we are fitting the tokenizer onto the list of sentences we already have. In the case of training with neural networks, what is going to happen if a word that has previously never been seen is encountered?\n",
    "\n",
    "```tokenizer.fit_on_texts(sentences)```\n",
    "\n",
    "```sequences = tokenizer.texts_to_sequences```\n",
    "\n",
    "What if a list  ```test_data``` were to be initialized as:\n",
    "\n",
    "```\n",
    "test_data = [\n",
    "    \"I really love my dog\",\n",
    "    \"My dog loves my manatee\",\n",
    "]\n",
    "```\n",
    "\n",
    "\"manatee\" here is a word that hasn't been encountered before.\n",
    "\n",
    "```test_seq = tokenizer.texts_to_sequences(test_data)```\n",
    "\n",
    "```print(test_seq)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data: list[str] = [\n",
    "    \"I really love my dog\",\n",
    "    \"My dog loves my manatee\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "print(test_seq)\n",
    "\n",
    "# the output is [[4, 2, 1, 3], [1, 3, 1]]\n",
    "\n",
    "# A five-word sentence (I really love my dog) only has 4 elements in its sequence form\n",
    "# This is so because the word \"really\" was not contained in the corpus used to build the tokenizer\n",
    "\n",
    "# Similarly, the second sentence: the words loves and manatee are not in the index because of which\n",
    "# only 3 tokens are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of points in the direction of needing a really massive word index to be able to handle large texts or sentences not in the training set.\n",
    "\n",
    "However, in order to not lose the length of the sentence, use the **OOV** trick\n",
    "\n",
    "OOV stands for **Out of Vocabulary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences: list[str] = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"Do you think my dog is amazing?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "print(test_seq) # still lost some meaning, but at least the sequences are the same length as the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Sentences of Varying Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"Do you think my dog is amazing?\",\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
